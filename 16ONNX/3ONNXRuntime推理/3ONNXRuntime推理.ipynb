{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX Runtime 库介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONNX Runtime 和 ONNX 是两个相关但功能不同的库：\n",
    "\n",
    "- ONNX 库的功能是定义神经网络模型的标准格式，支持跨框架模型导出和转换，专注于模型的设计、定义和优化。\n",
    "- ONNX Runtime 库功能是高性能推理引擎，用于执行和优化 ONNX 格式的模型，专注于在多种硬件上高效运行 ONNX 模型。  \n",
    "    \n",
    "如此设计拆分的原因有：\n",
    "\n",
    "- 职责分离: ONNX 负责模型定义，ONNX Runtime 负责模型执行，各自专注提高性能。\n",
    "- 灵活性: 拆分后，开发者可以灵活选择工具，适应不同需求。\n",
    "- 独立发展: 允许两个库独立更新和优化，满足各自领域的技术发展。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载 ONNX 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2025-03-22 14:20:21--  https://labfile.oss.aliyuncs.com/courses/40981/multi_io_model.onnx\n",
      "Resolving labfile.oss.aliyuncs.com (labfile.oss.aliyuncs.com)... 47.110.177.159\n",
      "Connecting to labfile.oss.aliyuncs.com (labfile.oss.aliyuncs.com)|47.110.177.159|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 71256 (70K) [application/octet-stream]\n",
      "Saving to: 'multi_io_model.onnx'\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 71% 9.01M 0s\n",
      "    50K .......... .........                                  100% 1.08M=0.02s\n",
      "\n",
      "2025-03-22 14:20:21 (2.94 MB/s) - 'multi_io_model.onnx' saved [71256/71256]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://labfile.oss.aliyuncs.com/courses/40981/multi_io_model.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort \n",
    "\n",
    "model_path = 'multi_io_model.onnx'\n",
    "session  = ort.InferenceSession(model_path)\n",
    "# ort.InferenceSession 用于创建一个推理会话，该会话将载入指定路径下的 ONNX 模型。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看模型输入输出格式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个输入形状：['batch_size', 10], 数据类型：tensor(float), 输入名称：input1\n",
      "第2个输入形状：['batch_size', 20], 数据类型：tensor(float), 输入名称：input2\n",
      "[<onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x000001BA0E4E0DF0>, <onnxruntime.capi.onnxruntime_pybind11_state.NodeArg object at 0x000001BA0E4E07B0>]\n"
     ]
    }
   ],
   "source": [
    "input_details = [input_ for input_ in session.get_inputs()]\n",
    "for idx,input_detail in enumerate(input_details):\n",
    "     print(f\"第{idx+1}个输入形状：{input_detail.shape}, 数据类型：{input_detail.type}, 输入名称：{input_detail.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为我们事先并不知道一个模型是单输入还是多输入，所以需要遍历所有输入，依次打印每个输入的形状和数据类型。\n",
    "\n",
    "- input_details.shape 返回输入的形状，它是一个列表或元组，表示每个维度的大小。\n",
    "- input_details.type 返回输入的数据类型。\n",
    "\n",
    "通过打印结果，我们可以看到这个模型需要输入两个 float 张量，节点 input1 尺寸为 ['batch_size', 10]，另一个 input2 为 ['batch_size', 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样的，获取输出格式，只需要把 .get_inputs 换成 .get_outputs 即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1个输出形状：['batch_size', 1], 数据类型：tensor(float), 输出名称：output1\n",
      "第2个输出形状：['batch_size', 2], 数据类型：tensor(float), 输出名称：output2\n"
     ]
    }
   ],
   "source": [
    "output_details = [input_ for input_ in session.get_outputs()]\n",
    "for idx, output_detail in enumerate(output_details):\n",
    "    print(f\"第{idx+1}个输出形状：{output_detail.shape}, 数据类型：{output_detail.type}, 输出名称：{output_detail.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.02000945],\n",
      "       [-0.03383649],\n",
      "       [-0.02177634]], dtype=float32), array([[-0.00301798, -0.20539439],\n",
      "       [ 0.03358935, -0.16653925],\n",
      "       [ 0.12497212, -0.15285939]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_data1 = np.random.rand(3,10).astype(np.float32)\n",
    "input_data2 = np.random.rand(3,20).astype(np.float32)\n",
    "\n",
    "outputs = session.run(None,{\"input1\":input_data1,\"input2\":input_data2})\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_data1 和 input_data2 是为模型提供的随机输入数据，形状分别为 (3, 10) 和 (3, 20)。\n",
    "session.run 函数执行模型推理，None 表示获取所有输出。输入名称 \"input1\"，\"input2\" 需要与模型的输入名称匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output1=[array([[ 0.02000945],\n",
      "       [-0.03383649],\n",
      "       [-0.02177634]], dtype=float32)]\n",
      "output2=[array([[-0.00301798, -0.20539439],\n",
      "       [ 0.03358935, -0.16653925],\n",
      "       [ 0.12497212, -0.15285939]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "output1 = session.run(['output1'], {\"input1\": input_data1, \"input2\": input_data2})\n",
    "output2 = session.run(['output2'], {\"input1\": input_data1, \"input2\": input_data2})\n",
    "print(f'{output1=}\\n{output2=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取中间结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import onnx\n",
    "onnx_model = onnx.load('multi_io_model.onnx')\n",
    "# 深拷贝模型的输出信息，ori_output 保存原始模型输出的结构，防止后续修改影响原有模型\n",
    "ori_output = copy.deepcopy(onnx_model.graph.output)\n",
    "\n",
    "for node in onnx_model.graph.node:\n",
    "    for output in node.output:\n",
    "         onnx_model.graph.output.extend([onnx.ValueInfoProto(name=output)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过循环遍历模型的每个节点，提取每个节点的输出，并将其添加到模型的 output 列表中，这样模型在推理时不仅会输出最终的结果，还会输出每一层的中间结果。\n",
    "\n",
    "然后我们为修改后的 ONNX 模型创建推理会话，并进行推理，使用有序字典顺序存储输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('output1', array([[ 0.02000945],\n",
      "       [-0.03383649],\n",
      "       [-0.02177634]], dtype=float32)), ('output2', array([[-0.00301798, -0.20539439],\n",
      "       [ 0.03358935, -0.16653925],\n",
      "       [ 0.12497212, -0.15285939]], dtype=float32)), ('/Constant_output_0', array(0.5, dtype=float32)), ('/Mul_output_0', array([[0.42464387, 0.47286236, 0.22205007, 0.02009887, 0.40676284,\n",
      "        0.22793245, 0.36089095, 0.46112078, 0.4685927 , 0.45767406],\n",
      "       [0.41848215, 0.02656515, 0.41577333, 0.43224296, 0.10359064,\n",
      "        0.34651738, 0.17580359, 0.32082206, 0.40584224, 0.00789164],\n",
      "       [0.20272185, 0.44108552, 0.495985  , 0.44066486, 0.48080394,\n",
      "        0.22549027, 0.41936097, 0.3724126 , 0.36917216, 0.11440618]],\n",
      "      dtype=float32)), ('/fc1/Gemm_output_0', array([[-0.05027089, -0.28285018, -0.10937285, ..., -0.4459226 ,\n",
      "         0.2738302 ,  0.55757916],\n",
      "       [-0.15196466, -0.15361848, -0.04035488, ..., -0.43479687,\n",
      "         0.40952694,  0.36501145],\n",
      "       [-0.16973707, -0.20146324, -0.07781276, ..., -0.43286324,\n",
      "         0.2867243 ,  0.4108109 ]], dtype=float32)), ('/Relu_output_0', array([[0.        , 0.        , 0.        , ..., 0.        , 0.2738302 ,\n",
      "        0.55757916],\n",
      "       [0.        , 0.        , 0.        , ..., 0.        , 0.40952694,\n",
      "        0.36501145],\n",
      "       [0.        , 0.        , 0.        , ..., 0.        , 0.2867243 ,\n",
      "        0.4108109 ]], dtype=float32)), ('/Constant_1_output_0', array(0.5, dtype=float32)), ('/Mul_1_output_0', array([[0.4593526 , 0.460948  , 0.14712866, 0.00249024, 0.02274782,\n",
      "        0.24839704, 0.4017341 , 0.12841196, 0.19445135, 0.08266871,\n",
      "        0.32613102, 0.49011123, 0.2022372 , 0.3646644 , 0.16401882,\n",
      "        0.45162806, 0.04099063, 0.22889864, 0.10077015, 0.13662615],\n",
      "       [0.46264145, 0.49815354, 0.0277223 , 0.3741833 , 0.2222111 ,\n",
      "        0.3372853 , 0.18225643, 0.43370536, 0.26726404, 0.09630881,\n",
      "        0.17131504, 0.45196545, 0.32928932, 0.3132346 , 0.3566266 ,\n",
      "        0.19082052, 0.01732417, 0.12307422, 0.03432129, 0.03611324],\n",
      "       [0.3078681 , 0.47227865, 0.45400596, 0.37184787, 0.47044015,\n",
      "        0.45402375, 0.43196973, 0.38189486, 0.15195794, 0.12878233,\n",
      "        0.34203413, 0.3401854 , 0.29018652, 0.12270407, 0.24256554,\n",
      "        0.18565674, 0.279455  , 0.42848077, 0.20732753, 0.33335632]],\n",
      "      dtype=float32)), ('/fc2/Gemm_output_0', array([[-0.14424011, -0.05996173,  0.2965889 , ..., -0.13039665,\n",
      "         0.42618537,  0.19541816],\n",
      "       [-0.10063515, -0.08109905,  0.23307438, ..., -0.1367752 ,\n",
      "         0.32225472,  0.23495945],\n",
      "       [-0.26950362, -0.21926808,  0.33530945, ..., -0.11071086,\n",
      "         0.33742774, -0.08760307]], dtype=float32)), ('/Relu_1_output_0', array([[0.        , 0.        , 0.2965889 , ..., 0.        , 0.42618537,\n",
      "        0.19541816],\n",
      "       [0.        , 0.        , 0.23307438, ..., 0.        , 0.32225472,\n",
      "        0.23495945],\n",
      "       [0.        , 0.        , 0.33530945, ..., 0.        , 0.33742774,\n",
      "        0.        ]], dtype=float32))])\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# 使用 ONNX Runtime 创建推理会话，SerializeToString 将模型序列化为字节流\n",
    "ort_session = ort.InferenceSession(onnx_model.SerializeToString())\n",
    "# 运行推理，使用 input_data1 和 input_data2 作为模型的输入\n",
    "ort_outs = ort_session.run(None, {\"input1\": input_data1, \"input2\": input_data2})\n",
    "\n",
    "# 获取模型中所有输出节点的名称\n",
    "outputs = [x.name for x in ort_session.get_outputs()]\n",
    "\n",
    "# 将输出的名称和对应的输出值以有序字典的形式组合，便于查找\n",
    "ort_outs = OrderedDict(zip(outputs, ort_outs))\n",
    "print(ort_outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lanqiao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
